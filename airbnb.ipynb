{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb Listing Price Prediction\n",
    "\n",
    "A portfolio-style end-to-end machine learning project for predicting Airbnb listing prices.\n",
    "## Objectives\n",
    "- Build a clean and reproducible modeling workflow.\n",
    "- Engineer meaningful listing-level features.\n",
    "- Compare model predictions against interpretable evaluation metrics.\n",
    "- Keep the notebook production-friendly using modular code and clear documentation.\n"
   "source": [
    "## 1) Setup and configuration\n"
   "execution_count": null,
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "TARGET_COLUMN = \"price\"\n",
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    \"\"\"Centralized project paths for easier maintenance.\"\"\"\n",
    "    data_path: Path = Path(\"/kaggle/input/airbnb-listings-project-data/Airbnb Data/Listings.csv\")\n",
    "paths = Paths()\n"
   "cell_type": "markdown",
   "metadata": {},
    "## 2) Data loading\n",
    "> The function below validates file existence and standardizes the loading process.\n"
   "metadata": {},
    "def load_data(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load listing data from CSV with light validation.\"\"\"\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Dataset not found at: {csv_path}\n",
    "\"\n",
    "            \"Update `Paths.data_path` to the correct location before running the notebook.\"\n",
    "        )\n",
    "    df_raw = pd.read_csv(csv_path, encoding=\"ISO-8859-1\", low_memory=False)\n",
    "    return df_raw\n",
    "\n",
    "df = load_data(paths.data_path)\n",
    "print(f\"Loaded dataset shape: {df.shape}\")\n",
    "df.head()\n"
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Data quality quick audit\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_dataframe(df_input: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return a compact quality report with dtype and missingness.\"\"\"\n",
    "    report = pd.DataFrame(\n",
    "        {\n",
    "            \"dtype\": df_input.dtypes.astype(str),\n",
    "            \"missing_count\": df_input.isna().sum(),\n",
    "            \"missing_pct\": (df_input.isna().mean() * 100).round(2),\n",
    "            \"n_unique\": df_input.nunique(dropna=False),\n",
    "        }\n",
    "    ).sort_values(\"missing_pct\", ascending=False)\n",
    "\n",
    "    return report\n",
    "\n",
    "quality_report = audit_dataframe(df)\n",
    "quality_report.head(20)\n"
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Feature engineering and cleanup\n",
    "\n",
    "Design goals:\n",
    "- Use robust transformations for skewed target values.\n",
    "- Add geospatial proxies for city-center accessibility.\n",
    "- Keep transformation logic in one reusable function.\n"
   "metadata": {},
    "def preprocess_airbnb_data(df_input: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Apply dataset-specific cleaning and feature engineering.\"\"\"\n",
    "\n",
    "    df_model = df_input.copy()\n",
    "\n",
    "    # Normalize price if loaded as string (e.g., \"$1,240.00\").\n",
    "    if df_model[TARGET_COLUMN].dtype == \"object\":\n",
    "        df_model[TARGET_COLUMN] = (\n",
    "            df_model[TARGET_COLUMN]\n",
    "            .astype(str)\n",
    "            .str.replace(r\"[$,]\", \"\", regex=True)\n",
    "            .astype(float)\n",
    "        )\n",
    "\n",
    "    # Basic geospatial features.\n",
    "    if {\"latitude\", \"longitude\"}.issubset(df_model.columns):\n",
    "        center_lat = df_model[\"latitude\"].mean()\n",
    "        center_lon = df_model[\"longitude\"].mean()\n",
    "        df_model[\"distance_from_center\"] = np.sqrt(\n",
    "            (df_model[\"latitude\"] - center_lat) ** 2\n",
    "            + (df_model[\"longitude\"] - center_lon) ** 2\n",
    "        )\n",
    "\n",
    "    if {\"distance_from_center\", \"accommodates\"}.issubset(df_model.columns):\n",
    "        df_model[\"distance_x_accommodates\"] = (\n",
    "            df_model[\"distance_from_center\"] * df_model[\"accommodates\"]\n",
    "        )\n",
    "    # Drop low-signal or very sparse columns when present.\n",
    "    columns_to_drop = [\"id\", \"name\", \"description\", \"district\", \"host_id\"]\n",
    "    available_to_drop = [col for col in columns_to_drop if col in df_model.columns]\n",
    "    df_model = df_model.drop(columns=available_to_drop)\n",
    "    return df_model\n",
    "\n",
    "\n",
    "df_model = preprocess_airbnb_data(df)\n",
    "print(f\"Post-processing shape: {df_model.shape}\")\n",
    "df_model.head()\n"
   "cell_type": "markdown",
   "metadata": {},
    "## 5) Train/test split and preprocessing pipeline\n"
   "metadata": {},
    "if TARGET_COLUMN not in df_model.columns:\n",
    "    raise KeyError(f\"Target column '{TARGET_COLUMN}' was not found in dataframe.\")\n",
    "# Remove rows with missing target to prevent training leakage.\n",
    "df_model = df_model[df_model[TARGET_COLUMN].notna()].copy()\n",
    "X = df_model.drop(columns=[TARGET_COLUMN])\n",
    "y = np.log1p(df_model[TARGET_COLUMN])\n",
    "numeric_features = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", RobustScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", model),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n"
   "cell_type": "markdown",
   "metadata": {},
    "## 6) Model training and evaluation\n"
   "metadata": {},
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log = pipeline.predict(X_test)\n",
    "\n",
    "y_test_actual = np.expm1(y_test)\n",
    "y_pred_actual = np.expm1(y_pred_log)\n",
    "\n",
    "metrics = {\n",
    "    \"MAE\": mean_absolute_error(y_test_actual, y_pred_actual),\n",
    "    \"RMSE\": mean_squared_error(y_test_actual, y_pred_actual, squared=False),\n",
    "    \"R2\": r2_score(y_test_actual, y_pred_actual),\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, index=[\"RandomForestRegressor\"]).T\n",
    "metrics_df.columns = [\"score\"]\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Baseline sanity checks and interpretation aids\n"
   "metadata": {},
    "baseline_pred = np.repeat(np.expm1(y_train).median(), len(y_test_actual))\n",
    "baseline_mae = mean_absolute_error(y_test_actual, baseline_pred)\n",
    "\n",
    "print(f\"Baseline MAE (median price): {baseline_mae:,.2f}\")\n",
    "print(f\"Model MAE: {metrics['MAE']:,.2f}\")\n",
    "print(f\"MAE improvement vs baseline: {baseline_mae - metrics['MAE']:,.2f}\")\n"
   "metadata": {},
    "def summarize_predictions(y_true: np.ndarray, y_pred: np.ndarray, n_rows: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Build a compact comparison table of predictions and residuals.\"\"\"\n",
    "    summary = pd.DataFrame(\n",
    "        {\n",
    "            \"actual_price\": y_true,\n",
    "            \"predicted_price\": y_pred,\n",
    "        }\n",
    "    )\n",
    "    summary[\"error\"] = summary[\"predicted_price\"] - summary[\"actual_price\"]\n",
    "    summary[\"abs_error\"] = summary[\"error\"].abs()\n",
    "    return summary.sort_values(\"abs_error\", ascending=False).head(n_rows)\n",
    "summarize_predictions(y_test_actual.values, y_pred_actual, n_rows=15)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Next steps\n",
    "\n",
    "- Perform feature importance analysis with permutation importance.\n",
    "- Add time-aware validation if booking or listing timestamps are available.\n",
    "- Compare tree-based models (e.g., XGBoost/LightGBM/CatBoost) with tuned hyperparameters.\n",
    "- Package preprocessing + model artifact for deployment.\n"
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', StandardScaler())])\n",
    "\n",
    "\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, numerical_columns),\n",
    "#         ('cat', categorical_transformer, categorical_columns)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('regressor', RandomForestRegressor(n_estimators=50, random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# y_pred_log = model.predict(X_test)\n",
    "# # y_pred = model.predict(X_test) \n",
    "# y_pred = np.expm1(y_pred_log)\n",
    "# y_test_original = np.expm1(y_test)\n",
    "\n",
    "\n",
    "# mae = mean_absolute_error(y_test_original, y_pred)\n",
    "# mse = mean_squared_error(y_test_original, y_pred)\n",
    "# rmse = mse ** 0.5\n",
    "# r2 = r2_score(y_test_original, y_pred)\n",
    "\n",
    "# print(\"MAE:\", mae)\n",
    "# print(\"RMSE:\", rmse)\n",
    "# print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df['price'].describe()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 15662539,
     "datasetId": 9468020,
     "sourceId": 14806917,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
